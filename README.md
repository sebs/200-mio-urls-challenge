# 200 million urls challenge

The "200 Million URLs Challenge" presents a problem in designing a datastorage  architecture capable of handling larger-scale data. The goal is to create a system that can  store 200 million URLs from 20,000 sources over a period of four years. Achieving this requires carefully balancing storage efficiency, performance, and scalability, all while managing the unique challenges posed by such a vast dataset.

To successfully tackle this challenge, the system must meet the following requirements:

- Store 200 Million URLs: The datastorage system should be capable of storing and managing up to 200 million unique URLs, ensuring that each is indexed efficiently.  
- URL Collection Over Four Years: URLs will be collected steadily over four years, with an average of 160,000 URLs added daily.
- Core Lookup Capabilities: The system should allow for quick verification of whether a given URL is already stored. Users must be able to retrieve URLs that have been added on the current day avoiding duplicates. The datastorage system should support counting URLs by their top-level domain (TLD). Users should be able to count URLs associated with a specific domain and subdomain. The system must allow for querying URLs based on specific GET parameters.
- Optional Features: The datastorage system can optionally store the link structure between URLs, enabling the retrieval of links originating from a specific URL and links pointing to a specific URL.

Start by forking the "200mio URLs Challenge" repository to your local environment.  Before diving into coding, brainstorm potential strategies for solving the problem. Focus on practical, implementable ideas that balance theoretical considerations with hands-on development. This challenge emphasizes practical coding over extensive theoretical analysis. Start by building and testing parts of the solution, iteratively refining your approach based on the outcomes. Remember, this is meant to be a fun and engaging exercise, so enjoy the process of problem-solving.
Designing a system to manage 200 million URLs comes with several significant challenges. Without access to 200 million URLs for testing, you'll need to generate synthetic data that mimics real-world scenarios to evaluate your system's performance. Given the sheer volume of data, the storage model must be efficient. URLs are complex entities, containing various components like protocol, domain, path, and query parameters, all of which may need separate indexing or processing. URLs on the web often don't follow strict standards, and issues like the order of GET arguments can complicate normalization and deduplication efforts. Traditional indexing methods might not be feasible due to the volume of data. Alternative strategies, such as partitioning or specialized data structures like Bloom filters, could offer more practical solutions. Inserting 160,000 URLs per day while maintaining responsive query performance is a significant challenge. You may need to implement batch processing, asynchronous insertion, or distributed datastorage systems to handle the load effectively. Estimating the required storage is crucial. This includes accounting for raw data as well as any additional metadata or indexing structures, ensuring that the system remains scalable over four years.
The "200 Million URLs Challenge" is a exercise in designing a scalable database system. By addressing the unique challenges of scale, complexity, and performance, you can propose a solution that balances efficiency, scalability, and maintainability. Embrace the process, experiment with different approaches, and enjoy the journey of trying to tinker this wicked problem.
